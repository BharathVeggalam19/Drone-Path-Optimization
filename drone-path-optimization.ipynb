{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11059202,"sourceType":"datasetVersion","datasetId":6890424}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:10:32.195550Z","iopub.execute_input":"2025-11-03T12:10:32.195862Z","iopub.status.idle":"2025-11-03T12:10:32.203575Z","shell.execute_reply.started":"2025-11-03T12:10:32.195838Z","shell.execute_reply":"2025-11-03T12:10:32.202994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, optimizers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:10:32.204901Z","iopub.execute_input":"2025-11-03T12:10:32.205137Z","iopub.status.idle":"2025-11-03T12:10:32.219219Z","shell.execute_reply.started":"2025-11-03T12:10:32.205122Z","shell.execute_reply":"2025-11-03T12:10:32.218569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your uploaded UAV dataset\ndata = pd.read_csv('/kaggle/input/uav-autonomous-navigation-dataset/uav_navigation_dataset.csv')\nprint(data.head())\n\n# Check for missing values\nprint(data.isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:10:32.219883Z","iopub.execute_input":"2025-11-03T12:10:32.220121Z","iopub.status.idle":"2025-11-03T12:10:32.262099Z","shell.execute_reply.started":"2025-11-03T12:10:32.220106Z","shell.execute_reply":"2025-11-03T12:10:32.261519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select useful features\nfeatures = ['latitude', 'longitude', 'altitude', 'lidar_distance', 'wind_speed', 'battery_level']\ntarget = 'obstacle_detected'\n\n# Normalize data for stability\nscaler = MinMaxScaler()\ndata[features] = scaler.fit_transform(data[features])\n\n# Convert to numpy arrays\nX = data[features].values\ny = data[target].values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:10:32.262753Z","iopub.execute_input":"2025-11-03T12:10:32.263030Z","iopub.status.idle":"2025-11-03T12:10:32.272020Z","shell.execute_reply.started":"2025-11-03T12:10:32.263004Z","shell.execute_reply":"2025-11-03T12:10:32.271389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DroneEnv(gym.Env):\n    def __init__(self, X, y):\n        super(DroneEnv, self).__init__()\n        self.X = X\n        self.y = y\n        self.n_samples = len(X)\n        \n        # Define state (6 features)\n        self.observation_space = spaces.Box(low=0, high=1, shape=(6,), dtype=np.float32)\n        # Define action space (6 directions + stay)\n        self.action_space = spaces.Discrete(7)\n        \n        self.current_step = 0\n        self.goal = np.array([0.9, 0.9, 0.5, 0.5, 0.5, 1.0])  # hypothetical goal\n\n    def reset(self):\n        self.current_step = np.random.randint(0, self.n_samples)\n        return self.X[self.current_step]\n\n    def step(self, action):\n        # Move randomly (simulation)\n        self.current_step = (self.current_step + np.random.randint(1, 10)) % self.n_samples\n        state = self.X[self.current_step]\n        obstacle = self.y[self.current_step]\n\n        # Reward logic\n        distance_to_goal = np.linalg.norm(state - self.goal)\n        reward = -distance_to_goal\n        if obstacle == 1:\n            reward -= 1\n        if distance_to_goal < 0.1:\n            reward += 10  # goal reached\n\n        done = distance_to_goal < 0.05 or self.current_step > self.n_samples - 2\n        return state, reward, done, {}\n\n    def render(self, mode='human'):\n        pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:10:32.273520Z","iopub.execute_input":"2025-11-03T12:10:32.273828Z","iopub.status.idle":"2025-11-03T12:10:32.282745Z","shell.execute_reply.started":"2025-11-03T12:10:32.273807Z","shell.execute_reply":"2025-11-03T12:10:32.282238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(state_size, action_size):\n    model = models.Sequential([\n        layers.Input(shape=(state_size,)),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(action_size, activation='linear')\n    ])\n\n    model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='mse')\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:10:32.283425Z","iopub.execute_input":"2025-11-03T12:10:32.283632Z","iopub.status.idle":"2025-11-03T12:10:32.299468Z","shell.execute_reply.started":"2025-11-03T12:10:32.283611Z","shell.execute_reply":"2025-11-03T12:10:32.298946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = DroneEnv(X, y)\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\nmodel = build_model(state_size, action_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:10:32.300205Z","iopub.execute_input":"2025-11-03T12:10:32.300417Z","iopub.status.idle":"2025-11-03T12:10:32.339605Z","shell.execute_reply.started":"2025-11-03T12:10:32.300395Z","shell.execute_reply":"2025-11-03T12:10:32.338927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_steps = 500\n\nfor e in range(episodes):\n    state = env.reset()\n    total_reward = 0\n    done = False\n    step = 0\n\n    while not done and step < max_steps:\n        # select action (epsilon-greedy)\n        if np.random.rand() < epsilon:\n            action = np.random.randint(action_size)\n        else:\n            q_values = model.predict(state.reshape(1, -1), verbose=0)\n            action = np.argmax(q_values[0])\n\n        # take action\n        next_state, reward, done, _ = env.step(action)\n        total_reward += reward\n\n        # compute target\n        target = reward\n        if not done:\n            target += gamma * np.max(model.predict(next_state.reshape(1, -1), verbose=0)[0])\n\n        q_values[0][action] = target\n\n        # update model safely\n        model.train_on_batch(state.reshape(1, -1), q_values)\n\n        # move forward\n        state = next_state\n        step += 1\n\n    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n    print(f\"Episode {e+1}/{episodes} - Reward: {total_reward:.2f} - Epsilon: {epsilon:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:10:32.346094Z","iopub.execute_input":"2025-11-03T12:10:32.346694Z","iopub.status.idle":"2025-11-03T13:19:48.169937Z","shell.execute_reply.started":"2025-11-03T12:10:32.346669Z","shell.execute_reply":"2025-11-03T13:19:48.169217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# You can extend this by saving episode rewards and plotting them\n# Example:\nplt.plot(range(episodes), [np.random.uniform(-10,10) for _ in range(episodes)])\nplt.title('Drone Path Optimization Training')\nplt.xlabel('Episode')\nplt.ylabel('Total Reward')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T13:19:48.171479Z","iopub.execute_input":"2025-11-03T13:19:48.171743Z","iopub.status.idle":"2025-11-03T13:19:48.758006Z","shell.execute_reply.started":"2025-11-03T13:19:48.171723Z","shell.execute_reply":"2025-11-03T13:19:48.757273Z"}},"outputs":[],"execution_count":null}]}